{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7051583c834f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Don't forget to change the path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "# Don't forget to change the path\n",
    "# for the .txt file\n",
    "with open(r'C:\\Users\\David\\skills_2.txt',encoding=\"utf8\") as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "content = [x.strip() for x in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim.models.phrases import Phraser, Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "for line in content:\n",
    "    tokens=word_tokenize(line)\n",
    "    tok=[w.lower() for w in tokens]\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    strpp=[w.translate(table) for w in tok]\n",
    "    words=[word for word in strpp if word.isalpha()]\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    words=[w for w in words if not w in stop_words]\n",
    "    x.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open ('common.txt') as f:\n",
    "    content2 = [row[0] for row in csv.reader(f,delimiter='\\n')]    \n",
    "ntexts=[]\n",
    "l=len(texts)\n",
    "print(f\"Previous Length\\t= {l}\")\n",
    "for j in range(l):\n",
    "    s=texts[j]\n",
    "    res = [i for i in s if i not in content2]\n",
    "    ntexts.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ntexts[7])\n",
    "print(f\"Current Length\\t= {len(ntexts)}\")\n",
    "texts=ntexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content=texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "common_terms = [\"of\", \"with\", \"without\", \"and\", \"or\", \"the\", \"a\"]\n",
    "x=ntexts\n",
    "\n",
    "# Create relevant phrases from the list of tokenized sentences\n",
    "phrases = Phrases(x, min_count=1, threshold=1)\n",
    "\n",
    "# The Phraser object is used from now on to transform sentences\n",
    "bigram = Phraser(phrases)\n",
    "\n",
    "all_sentences = bigram[x]\n",
    "model = gensim.models.Word2Vec(all_sentences, vector_size=5000, min_count=2, workers=4, window=4)\n",
    "model.save(\"final.model\")\n",
    "wrds = model.wv.key_to_index.keys()\n",
    "print(wrds)\n",
    "print(len(wrds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add new words to the Word2Vec model\n",
    "\n",
    "# model = Word2Vec.load(\"your_model_path\")\n",
    "\n",
    "# new_words = [\"quick_typing\"]\n",
    "# model.build_vocab([new_words], update=True)\n",
    "\n",
    "# model.train([new_words], total_examples=len(new_words), epochs=model.epochs)\n",
    "# similar_words = model.wv.most_similar(\"deep_learning\")\n",
    "# print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=model.wv.most_similar(\"machine_learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "import collections\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "### Import en_core_web_sm to leverage\n",
    "### capabilities in NLP Pipelining\n",
    "\n",
    "### In your environment, do these:\n",
    "# (optional) pip install en_core_web_sm\n",
    "# pip install spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading the resumes in a folder\n",
    "mypath=r'C:\\Users\\David\\__resumes'\n",
    "# Path for the files\n",
    "onlyfiles = [os.path.join(mypath, f) for f in os.listdir(mypath) if os.path.isfile(os.path.join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdfextract(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        pdf_reader = PyPDF2.PdfReader(f)\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "    \n",
    "        for page_number in range(num_pages):\n",
    "            page = pdf_reader.pages[page_number]\n",
    "\n",
    "            page_text = page.extract_text()\n",
    "        \n",
    "    encoded_text = page_text.encode('utf-8')\n",
    "    # print(encoded_text)\n",
    "    print(\"----------------Testing-Line---------------\")\n",
    "    return encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_words=[k[0] for k in model.wv.most_similar(\"machine_learning\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigram(words):\n",
    "    common_terms = [\"of\", \"with\", \"without\", \"and\", \"or\", \"the\", \"a\"]\n",
    "    x=words.split()\n",
    "# Create the relevant phrases from the list of sentences:\n",
    "    phrases = Phrases(x, common_terms=common_terms)\n",
    "# The Phraser object is used from now on to transform sentences\n",
    "    bigram = Phraser(phrases)\n",
    "# Applying the Phraser to transform our sentences is simply\n",
    "    all_sentences = list(bigram[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_profile(file):\n",
    "    model=Word2Vec.load(\"final.model\")\n",
    "    text = str(pdfextract(file))\n",
    "    text = text.replace(\"\\\\n\", \"\")\n",
    "    text = text.lower()\n",
    "    #print(text)\n",
    "    #text=create_bigram(text)\n",
    "    #print(text)\n",
    "    #below is the csv where we have all the keywords, you can customize your own\n",
    "    #keyword_dictionary = pd.read_csv(r'C:\\Users\\dell\\Desktop\\New folder\\ML_CS\\NLP\\technical_skills.csv')\n",
    "    stats = [nlp(text[0]) for text in model.wv.most_similar(\"statistics\")]\n",
    "    NLP = [nlp(text[0]) for text in model.wv.most_similar(\"language\")]\n",
    "    ML = [nlp(text[0]) for text in model.wv.most_similar(\"machine_learning\")]\n",
    "    DL = [nlp(text[0]) for text in model.wv.most_similar(\"deeplearning\")]\n",
    "    python = [nlp(text[0]) for text in model.wv.most_similar(\"python\")]\n",
    "    Data_Engineering = [nlp(text[0]) for text in model.wv.most_similar(\"data\")]\n",
    "    print(\"*******************************************\")\n",
    "    #print(stats_words,NLP_words)\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    matcher.add('Stats', None, *stats)\n",
    "    matcher.add('NLP', None, *NLP)\n",
    "    matcher.add('ML', None, *ML)\n",
    "    matcher.add('DL', None, *DL)\n",
    "    matcher.add('Python', None, *python)\n",
    "    matcher.add('DE', None, *Data_Engineering)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    d = []  \n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        rule_id = nlp.vocab.strings[match_id]  # get the unicode I\n",
    "        span = doc[start : end]               # get the matched slice of the doc\n",
    "        d.append((rule_id, span.text))      \n",
    "    keywords = \"\\n\".join(f'{i[0]} {i[1]} ({j})' for i,j in Counter(d).items())\n",
    "    print(\"KEYWORDS\")\n",
    "    print(keywords)\n",
    "    \n",
    "    ## convertimg string of keywords to dataframe\n",
    "    df = pd.read_csv(StringIO(keywords),names = ['Keywords_List'])\n",
    "    df1 = pd.DataFrame(df.Keywords_List.str.split(' ', n=1).tolist(), columns=['Subject', 'Keyword'])\n",
    "    df2 = pd.DataFrame(df1.Keyword.str.split('(', n=1).tolist(), columns=['Keyword', 'Count'])\n",
    "    # df1 = pd.DataFrame(df.Keywords_List.str.split(' ',1).tolist(),columns = ['Subject','Keyword'])\n",
    "    # df2 = pd.DataFrame(df1.Keyword.str.split('(',1).tolist(),columns = ['Keyword', 'Count'])\n",
    "    df3 = pd.concat([df1['Subject'],df2['Keyword'], df2['Count']], axis =1) \n",
    "    df3['Count'] = df3['Count'].apply(lambda x: x.rstrip(\")\"))\n",
    "    print(\"********************DF********************\")\n",
    "    print(df)\n",
    "    \n",
    "    base = os.path.basename(file)\n",
    "    filename = os.path.splitext(base)[0]\n",
    "    \n",
    "       \n",
    "    name = filename.split('_')\n",
    "    print(name)\n",
    "    name2 = name[0]\n",
    "    name2 = name2.lower()\n",
    "    ## converting str to dataframe\n",
    "    name3 = pd.read_csv(StringIO(name2),names = ['Candidate Name'])\n",
    "    \n",
    "    dataf = pd.concat([name3['Candidate Name'], df3['Subject'], df3['Keyword'], df3['Count']], axis = 1)\n",
    "    dataf['Candidate Name'].fillna(dataf['Candidate Name'].iloc[0], inplace = True)\n",
    "    print(\"******************DATAF**************\")\n",
    "    print(dataf)\n",
    "\n",
    "    return(dataf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code to execute the above functions \n",
    "final_db=pd.DataFrame()\n",
    "i=0\n",
    "while i < len(onlyfiles):\n",
    "    file=onlyfiles[i]\n",
    "    dat=create_profile(file)\n",
    "\n",
    "    final_db = pd.concat([final_db, dat], ignore_index=True)\n",
    "    # final_db=final_db.append(dat)\n",
    "    i+=1\n",
    "    #print(final_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to count words under each category and visualize it through MAtplotlib\n",
    "final_db2 = final_db['Keyword'].groupby([final_db['Candidate Name'], final_db['Subject']]).count().unstack()\n",
    "final_db2.reset_index(inplace = True)\n",
    "final_db2.fillna(0,inplace=True)\n",
    "candidate_data = final_db2.iloc[:,1:]\n",
    "candidate_data.index = final_db2['Candidate Name']\n",
    "\n",
    "# The candidate profile in a csv format\n",
    "cand=candidate_data.to_csv('candidate_profile.csv')\n",
    "cand_profile=pd.read_csv('candidate_profile.csv')\n",
    "cand_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "ax = candidate_data.plot.barh(title=\"Keywords in Resume\", legend=False, figsize=(25,7), stacked=True)\n",
    "skills = []\n",
    "for j in candidate_data.columns:\n",
    "    for i in candidate_data.index:\n",
    "        skill = str(j)+\": \" + str(candidate_data.loc[i][j])\n",
    "        skills.append(skill)\n",
    "patches = ax.patches\n",
    "for skill, rect in zip(skills, patches):\n",
    "    width = rect.get_width()\n",
    "    if width > 0:\n",
    "        x = rect.get_x()\n",
    "        y = rect.get_y()\n",
    "        height = rect.get_height()\n",
    "        ax.text(x + width/2., y + height/2., skill, ha='center', va='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
